{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saravatpt/llmops_samples/blob/main/tuning_text_bison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "# Tuning and deploy a foundation model\n",
        "\n",
        "> **NOTE:** This notebook uses the PaLM generative model, which will reach its [discontinuation date in October 2024](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text#model_versions). Please refer to [this updated notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/gemini_supervised_tuning_qa.ipynb) for a version which uses the latest Gemini model.\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/tuning_text_bison.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/tuning_text_bison.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/tuning/tuning_text_bison.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/tuning_text_bison.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/tuning_text_bison.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/tuning_text_bison.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/X_logo_2023_original.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/tuning_text_bison.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/tuning_text_bison.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>            \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db178f5f0985"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Erwin Huizenga](https://github.com/erwinh85) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "Creating an LLM requires massive amounts of data, significant computing resources, and specialized skills. On Vertex AI, tuning allows you to customize a foundation model for more specific tasks or knowledge domains.\n",
        "\n",
        "While the prompt design is excellent for quick experimentation, if training data is available, you can achieve higher quality by tuning the model. Tuning a model enables you to customize the model response based on examples of the task you want the model to perform.\n",
        "\n",
        "For more details on tuning have a look at the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### Objective\n",
        "\n",
        "This tutorial teaches you how to tune a foundational model on new unseen data and you will use the following Google Cloud products:\n",
        "\n",
        "- Vertex AI Generative AI Studio\n",
        "- Vertex AI Pipelines\n",
        "- Vertex AI Model Registry\n",
        "- Vertex AI Endpoints\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Get training data from BQ and generate a JSONL file\n",
        "- Upload training data\n",
        "- Create a pipeline job\n",
        "- Inspect your model on Vertex AI Model Registry\n",
        "- Get predictions from your tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CZvFRbIaalF"
      },
      "source": [
        "### Quota\n",
        "**important**: Tuning the text-bison model uses the TPU `v3-8` training resources and the accompanying quotas from your Google Cloud project. Each project has a default quota of eight `v3-8` cores, which allows for one to two concurrent tuning jobs. If you want to run more concurrent jobs you need to request additional quota via the [Quotas page](https://console.cloud.google.com/iam-admin/quotas)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q2bKpVjaalF"
      },
      "source": [
        "### Costs\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI Generative AI Studio\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acBlvcGFaalF"
      },
      "source": [
        "### Install Vertex AI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEtR1xyRaalG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "89138406-1ef3-4ba4-8855-c3e0e2ef6a00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.10/dist-packages (1.74.0)\n",
            "Collecting google-cloud-aiplatform\n",
            "  Downloading google_cloud_aiplatform-1.75.0-py2.py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.10/dist-packages (3.25.0)\n",
            "Collecting google-cloud-bigquery\n",
            "  Downloading google_cloud_bigquery-3.27.0-py2.py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting sequence-evaluate\n",
            "  Downloading sequence_evaluate-0.0.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (4.25.5)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (24.2)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.14.0)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.0.6)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.10.3)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (0.16)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.7.2)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.32.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.66.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.68.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2024.12.14)\n",
            "Requirement already satisfied: numpy<3,>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.26.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading google_cloud_aiplatform-1.75.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_bigquery-3.27.0-py2.py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m240.1/240.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sequence_evaluate-0.0.3-py3-none-any.whl (7.2 kB)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: sequence-evaluate, rouge, google-cloud-bigquery, google-cloud-aiplatform\n",
            "\u001b[33m  WARNING: The script rouge is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.75.0 google-cloud-bigquery-3.27.0 rouge-1.0.1 sequence-evaluate-0.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "65eb509da0464bac9d72cac15329a06e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%pip install google-cloud-aiplatform google-cloud-bigquery sequence-evaluate sentence-transformers rouge --upgrade --user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAMVnZC9aalG"
      },
      "source": [
        "**Colab only:** Uncomment the following cell to restart the kernel or use the restart button. For Vertex AI Workbench you can restart the terminal using the button on top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdQC6wcuaalG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d594f7ad-6360-4b1f-c050-017636a51528"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LlxsZrWaalG"
      },
      "source": [
        "### Authenticating your notebook environment\n",
        "* If you are using **Colab** to run this notebook, uncomment the cell below and continue.\n",
        "* If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh-QANoIaalG"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW8qtGsmaalG"
      },
      "source": [
        "### BigQuery IAM\n",
        "Now you need to add permissions to the service account:\n",
        "- Go to the [IAM page](https://console.cloud.google.com/iam-admin/) in the console\n",
        "- Look for the default compute service account. It should look something like this: `<project-number>-compute@developer.gserviceaccount.com`\n",
        "- Assign the default compute service account with `bigquery.user`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmhnHOjlaalH"
      },
      "source": [
        "### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`. Otherwise, check the support page: Locate the [project ID](https://support.google.com/googleapi/answer/7014113). Please update `PROJECT_ID` below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8nXkkYxaalH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ccd1ce1-1c9d-4950-d2ee-7b8d9d5561ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "PROJECT_ID = \"gen-lang-client-0302405510\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrsmSjICaalH"
      },
      "source": [
        "### Create a bucket\n",
        "Now you have to create a bucket that we will use to store our tuning data. To avoid name collisions between users on resources created, you generate a UUID for each instance session and append it to the name of the resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiKRZOgqaalH"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of a specified length(default=8)\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D28-KrtaalH"
      },
      "source": [
        "Choose a bucket name and update the `BUCKET_NAME` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxRSNVCYaalH"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"tuning-text-bison\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpjqMRc-aalH"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"tuning-text-bison\":\n",
        "    BUCKET_NAME = \"vertex-\" + UUID\n",
        "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtJg8ILPaalH"
      },
      "source": [
        "Only if your bucket doesn't already exist: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSRiXkavaalH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67eba2e4-22b9-43f2-e584-d24778d20312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://vertex-eg6hgxqq/...\n"
          ]
        }
      ],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNL0oqUJaalH"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ygbBJuke-uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leJFL5oIaalH"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E7pfl6sjzh_"
      },
      "source": [
        "**Colab only**: Run the following cell to initialize the Vertex AI SDK. For Vertex AI Workbench, you don't need to run this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXlNPFmGjzh_"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRUOFELefqf1"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform, bigquery\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from vertexai.language_models import TextGenerationModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdtNETYxaalH"
      },
      "source": [
        "## Tune your Model\n",
        "\n",
        "Now it's time for you to create a tuning job. Tune a foundation model by creating a pipeline job using Generative AI Studio, cURL, or the Python SDK. In this notebook, we will be using the Python SDK. You will be using a Q&A with a context dataset in JSON format.\n",
        "\n",
        "### Training Data\n",
        "üíæ Your model tuning dataset must be in a JSONL format where each line contains a single training example. You must make sure that you include instructions.\n",
        "\n",
        "You will use the StackOverflow data on BigQuery Public Datasets, limiting to questions with the `python` tag, and accepted answers for answers since 2020-01-01."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Puc3jl8QaalI"
      },
      "source": [
        "First create a helper function to let you easily query BigQuery and return the results as a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg60aUgvaalI"
      },
      "outputs": [],
      "source": [
        "def run_bq_query(sql: str) -> str | pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run a BigQuery query and return the job ID or result as a DataFrame\n",
        "    Args:\n",
        "        sql: SQL query, as a string, to execute in BigQuery\n",
        "    Returns:\n",
        "        df: DataFrame of results from query,  or error, if any\n",
        "    \"\"\"\n",
        "\n",
        "    bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "    # Try dry run before executing query to catch any errors\n",
        "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
        "    bq_client.query(sql, job_config=job_config)\n",
        "\n",
        "    # If dry run succeeds without errors, proceed to run query\n",
        "    job_config = bigquery.QueryJobConfig()\n",
        "    client_result = bq_client.query(sql, job_config=job_config)\n",
        "\n",
        "    job_id = client_result.job_id\n",
        "\n",
        "    # Wait for query/job to finish running. then get & return DataFrame\n",
        "    df = client_result.result().to_arrow().to_pandas()\n",
        "    print(f\"Finished job_id: {job_id}\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BydoFfTaalI"
      },
      "source": [
        "Next define the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VTaovLtaalI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "83992628-6f8e-49ae-a967-643ce12454c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished job_id: 20c376b8-1e88-4729-bb16-ca121701f010\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          input_text  \\\n",
              "0  Pytorch matrix size issue does not multiply<p>...   \n",
              "1  Python struct not accepting 0xf031 as unsigned...   \n",
              "2  Inserting multiple columns from one csv file i...   \n",
              "3  Update a row in a Django data base based on a ...   \n",
              "4  Python & Sympy : How can we adjust the coordin...   \n",
              "\n",
              "                                         output_text  \n",
              "0  <p>I have tried your model and your calculatio...  \n",
              "1  <p>You're seeing what you should be seeing. Th...  \n",
              "2  <p>You can try <code>pd.concat</code> on colum...  \n",
              "3  <pre class=\"lang-py prettyprint-override\"><cod...  \n",
              "4  <p>There is the <code>aspect_ratio</code> keyw...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e175d63d-08f6-4dbc-bfbe-88a248b305fc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_text</th>\n",
              "      <th>output_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pytorch matrix size issue does not multiply&lt;p&gt;...</td>\n",
              "      <td>&lt;p&gt;I have tried your model and your calculatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Python struct not accepting 0xf031 as unsigned...</td>\n",
              "      <td>&lt;p&gt;You're seeing what you should be seeing. Th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Inserting multiple columns from one csv file i...</td>\n",
              "      <td>&lt;p&gt;You can try &lt;code&gt;pd.concat&lt;/code&gt; on colum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Update a row in a Django data base based on a ...</td>\n",
              "      <td>&lt;pre class=\"lang-py prettyprint-override\"&gt;&lt;cod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Python &amp; Sympy : How can we adjust the coordin...</td>\n",
              "      <td>&lt;p&gt;There is the &lt;code&gt;aspect_ratio&lt;/code&gt; keyw...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e175d63d-08f6-4dbc-bfbe-88a248b305fc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e175d63d-08f6-4dbc-bfbe-88a248b305fc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e175d63d-08f6-4dbc-bfbe-88a248b305fc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3f9471dd-2d89-442e-b75e-4d0dd3ff3973\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3f9471dd-2d89-442e-b75e-4d0dd3ff3973')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3f9471dd-2d89-442e-b75e-4d0dd3ff3973 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"input_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"get id inside circle with python and mysql<p>I have two points A(lat1,lng1) and B(lat2,lng2) and I want to get all the ids of a MySQL table inside those two points (A and B as a diameter of a circle ).\\nHow can I do that? I am a little bit stuck\\nthank you very much\\n<a href=\\\"https://i.stack.imgur.com/flr3z.png\\\" rel=\\\"nofollow noreferrer\\\">see this image here</a></p>\",\n          \"How can I remove everything after a specific text present in html ? Using python and beautifulsoup4<p>I'm trying to scrape wikipedia. I wish to get only the desired data and discard everthing which is unncessary such as <strong>See also</strong>, <strong>References</strong>, etc.</p>\\n<pre><code>&lt;h2&gt;\\n     &lt;span class=&quot;mw-headline&quot; id=&quot;See_also&quot;&gt;See also&lt;/span&gt;\\n&lt;/h2&gt;\\n&lt;ul&gt;\\n     &lt;li&gt;&lt;a href=&quot;/wiki/List_of_adaptations_of_works_by_Stephen_King&quot; title=&quot;List of adaptations of works by Stephen King&quot;&gt;List of adaptations of works by Stephen King&lt;/a&gt;&lt;/li&gt;\\n     &lt;li&gt;&lt;a href=&quot;/wiki/Castle_Rock_(Stephen_King)&quot; title=&quot;Castle Rock (Stephen King)&quot;&gt;Castle Rock (Stephen King)&lt;/a&gt;&lt;/li&gt;\\n     &lt;li&gt;&lt;a href=&quot;/wiki/Charles_Scribner%27s_Sons&quot; title=&quot;Charles Scribner&amp;#39;s Sons&quot;&gt;Charles Scribner's Sons&lt;/a&gt; (aka Scribner)&lt;/li&gt;\\n     &lt;li&gt;&lt;a href=&quot;/wiki/Derry_(Stephen_King)&quot; title=&quot;Derry (Stephen King)&quot;&gt;Derry (Stephen King)&lt;/a&gt;&lt;/li&gt;\\n     &lt;li&gt;&lt;a href=&quot;/wiki/Dollar_Baby&quot; title=&quot;Dollar Baby&quot;&gt;Dollar Baby&lt;/a&gt;&lt;/li&gt;\\n     &lt;li&gt;&lt;a href=&quot;/wiki/Jerusalem%27s_Lot_(Stephen_King)&quot; title=&quot;Jerusalem&amp;#39;s Lot (Stephen King)&quot;&gt;Jerusalem's Lot (Stephen King)&lt;/a&gt;&lt;/li&gt;\\n     &lt;li&gt;&lt;i&gt;&lt;a href=&quot;/wiki/Haven_(TV_series)&quot; title=&quot;Haven (TV series)&quot;&gt;Haven&lt;/a&gt;&lt;/i&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n</code></pre>\\n<p>As shown in the above HTML. If I find <strong>See also</strong> in <strong>h2</strong> tag, I want to delete everything which is followed by it. unordered list in this case.</p>\",\n          \"Grab a specific item in Json data using Python<p>I'm currently writing a script to find emails based on the domain name using Hunter.io API</p>\\n<p>The thing is that my script returns me a JSON with a lot's a details and I only want the mail address in it.</p>\\n<p>Here's my code :</p>\\n<pre><code> &quot;&quot;&quot;script to find emails based on the domain name using Hunter.io&quot;&quot;&quot;\\n\\nimport requests # To make get and post requests to APIs\\nimport json # To deal with json responses to APIs\\nfrom pprint import pprint # To pretty print json info in a more readable format\\nfrom pyhunter import PyHunter # Using the hunter module\\n\\n# Global variables\\n\\nhunter_api_key = &quot;API KEY&quot; # API KEY \\ncontacts = [] # list where we'll store our contacts\\ncontact_info = {} # dictionary where we'll store each contact information\\nlimit = 1 # Limit of emails adresses pulled by the request\\nvalue = &quot;value&quot; # &lt;- seems to be the key in Hunter's API of the email adress founded\\n\\n# TODO: Section A - Ask Hunter to find emails based on the domain name\\n\\ndef get_email_from_hunter(domain_name,limit):\\n      \\n    url = &quot;https://api.hunter.io/v2/domain-search&quot;\\n\\n    params = {\\n        &quot;domain&quot; : domain_name,\\n        &quot;limit&quot; : 1,\\n        &quot;api_key&quot; : hunter_api_key,\\n    }\\n\\n    response = requests.get(url, params= params,)\\n\\n    json_data = response.json()\\n\\n    email_adress = json_data[&quot;data&quot;][&quot;emails&quot;] # &lt;- I have to find witch is the good key in order to return only the mail adress\\n    #pprint(email_adress)\\n\\n    contact_info[&quot;email_adress&quot;] = email_adress\\n    contact_info[&quot;domain_name&quot;] = domain_name\\n    pprint(contact_info)\\n    return contact_info\\n\\nget_email_from_hunter(&quot;intercom.io&quot;,&quot;1&quot;)\\n</code></pre>\\n<p>and here's the JSON returned :</p>\\n<p><a href=\\\"https://i.stack.imgur.com/dYXR2.png\\\" rel=\\\"nofollow noreferrer\\\">JSON exemple extracted from the documentation</a></p>\\n<p>Thanks per advance for the help provided :)</p>\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"<p>For example I have two points that I want to search points within those two defined points. I calculate the distance between those two points and divided by two to get the radius. Latitude of center = (lat_point1 + lat_point2)/2 and same for longitude. Then I search from this center all the points within the radius.</p>\\n<pre><code>SET center = ST_GeomFromText( 'POINT(35.322825 139.52769)', 4326 ); #center  between spot_1 and 2\\nSET spot_1 = ST_GeomFromText( 'POINT(35.30633 139.50644 )', 4326 );\\nSET spot_2 = ST_GeomFromText( 'POINT(35.33932 139.54894 )', 4326 );\\n\\nSELECT id, lat, lng, ROUND(ST_Distance(geometry, center, 'kilometre'), 2) FROM points where ST_Distance(spot_2 ,geometry, 'kilometre') &lt;=radius\\n</code></pre>\",\n          \"<p>You can use CSS selector with <code>~</code> to select right elements to extract:</p>\\n<pre class=\\\"lang-py prettyprint-override\\\"><code>from bs4 import BeautifulSoup\\n\\ntxt = '''\\n&lt;div&gt;This I want to keep&lt;/div&gt;\\n&lt;h2&gt;\\n     &lt;span class=&quot;mw-headline&quot; id=&quot;See_also&quot;&gt;See also&lt;/span&gt;\\n&lt;/h2&gt;\\n&lt;ul&gt;\\n     &lt;li&gt;&lt;a href=&quot;/wiki/List_of_adaptations_of_works_by_Stephen_King&quot; title=&quot;List of adaptations of works by Stephen King&quot;&gt;List of adaptations of works by Stephen King&lt;/a&gt;&lt;/li&gt;\\n     &lt;li&gt;&lt;a href=&quot;/wiki/Castle_Rock_(Stephen_King)&quot; title=&quot;Castle Rock (Stephen King)&quot;&gt;Castle Rock (Stephen King)&lt;/a&gt;&lt;/li&gt;\\n     &lt;li&gt;&lt;a href=&quot;/wiki/Charles_Scribner%27s_Sons&quot; title=&quot;Charles Scribner&amp;#39;s Sons&quot;&gt;Charles Scribner's Sons&lt;/a&gt; (aka Scribner)&lt;/li&gt;\\n     &lt;li&gt;&lt;a href=&quot;/wiki/Derry_(Stephen_King)&quot; title=&quot;Derry (Stephen King)&quot;&gt;Derry (Stephen King)&lt;/a&gt;&lt;/li&gt;\\n     &lt;li&gt;&lt;a href=&quot;/wiki/Dollar_Baby&quot; title=&quot;Dollar Baby&quot;&gt;Dollar Baby&lt;/a&gt;&lt;/li&gt;\\n     &lt;li&gt;&lt;a href=&quot;/wiki/Jerusalem%27s_Lot_(Stephen_King)&quot; title=&quot;Jerusalem&amp;#39;s Lot (Stephen King)&quot;&gt;Jerusalem's Lot (Stephen King)&lt;/a&gt;&lt;/li&gt;\\n     &lt;li&gt;&lt;i&gt;&lt;a href=&quot;/wiki/Haven_(TV_series)&quot; title=&quot;Haven (TV series)&quot;&gt;Haven&lt;/a&gt;&lt;/i&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n'''\\n\\nsoup = BeautifulSoup(txt, 'html.parser')\\n\\nfor tag in soup.select('h2:contains(&quot;See also&quot;) ~ *, h2:contains(&quot;See also&quot;)'):\\n    tag.extract()\\n\\nprint(soup)\\n</code></pre>\\n<p>Prints:</p>\\n<pre class=\\\"lang-html prettyprint-override\\\"><code>&lt;div&gt;This I want to keep&lt;/div&gt;\\n</code></pre>\\n<hr />\\n<p>NOTE: Newer versions of <code>bs4</code> use <code>:-soup-contains</code> instead of <code>:contains</code></p>\",\n          \"<pre><code>email_addresses = [item['value'] for item in json_data[&quot;data&quot;][&quot;emails&quot;]]\\n</code></pre>\\n<p>note, not tested as you post image, not json data as text.</p>\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df = run_bq_query(\n",
        "    \"\"\"SELECT\n",
        "    CONCAT(q.title, q.body) as input_text,\n",
        "    a.body AS output_text\n",
        "FROM\n",
        "    `bigquery-public-data.stackoverflow.posts_questions` q\n",
        "JOIN\n",
        "    `bigquery-public-data.stackoverflow.posts_answers` a\n",
        "ON\n",
        "    q.accepted_answer_id = a.id\n",
        "WHERE\n",
        "    q.accepted_answer_id IS NOT NULL AND\n",
        "    REGEXP_CONTAINS(q.tags, \"python\") AND\n",
        "    a.creation_date >= \"2020-01-01\"\n",
        "LIMIT\n",
        "    10000\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYUg8cBbaalJ"
      },
      "source": [
        "There should be 10k questions and answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FqbVHoeaalJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f63f2058-bf32-4b42-a47d-d8c84e6379cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ],
      "source": [
        "print(len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OftmoPZ6aalJ"
      },
      "source": [
        "Lets split the data into training and evaluation. For Extractive Q&A tasks we advise 100+ training examples. In this case you will use 800."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXqBwSwaaalJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0d4754-1a8f-4d44-cd86-6868c9ff1d0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8000\n",
            "250\n"
          ]
        }
      ],
      "source": [
        "# split is set to 80/20\n",
        "train, evaluation = train_test_split(df, test_size=0.2)\n",
        "evaluation = evaluation.sample(n=250, random_state=1)\n",
        "print(len(train))\n",
        "print(len(evaluation))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf-q8TpnaalJ"
      },
      "source": [
        "For tuning, the training data first needs to be converted into a JSONL format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqRbOwzEaalJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4734247-b828-4ef7-e72c-7000a61897d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length: 23555736\n",
            "{\"input_text\":\"How can I find out how much storage it would take to store every number between 1 and\n"
          ]
        }
      ],
      "source": [
        "tune_jsonl = train.to_json(orient=\"records\", lines=True)\n",
        "\n",
        "print(f\"Length: {len(tune_jsonl)}\")\n",
        "print(tune_jsonl[0:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r04PWISCaalJ"
      },
      "source": [
        "Next, you can write it to a local JSONL before transferring it to Google Cloud Storage (GCS)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXVV9c0HaalJ"
      },
      "outputs": [],
      "source": [
        "training_data_filename = \"tune_data_stack_overflow_python_qa.jsonl\"\n",
        "\n",
        "with open(training_data_filename, \"w\") as f:\n",
        "    f.write(tune_jsonl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHS1lDIrrfQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef6c9ef2-643e-41d8-d02f-3dc061e044ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length: 702058\n",
            "{\"input_text\":\"How to Unpack Dictionary in Column Dataframe Pandas<p>Stackoverflow, please do your m\n"
          ]
        }
      ],
      "source": [
        "tune_jsonl = evaluation.to_json(orient=\"records\", lines=True)\n",
        "\n",
        "print(f\"Length: {len(tune_jsonl)}\")\n",
        "print(tune_jsonl[0:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eULfrv2rTjO"
      },
      "outputs": [],
      "source": [
        "evaluation_data_filename = \"tune_eval_data_stack_overflow_python_qa.jsonl\"\n",
        "\n",
        "with open(evaluation_data_filename, \"w\") as f:\n",
        "    f.write(tune_jsonl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV8Wxz7JaalN"
      },
      "source": [
        "You can then export the local file to GCS, so that it can be used by Vertex AI for the tuning job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDDLHac5aalN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7a78827-0e61-4880-8665-3d5df6d2739a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file://tune_data_stack_overflow_python_qa.jsonl [Content-Type=application/octet-stream]...\n",
            "Copying file://tune_eval_data_stack_overflow_python_qa.jsonl [Content-Type=application/octet-stream]...\n",
            "| [2 files][ 23.1 MiB/ 23.1 MiB]                                                \n",
            "Operation completed over 2 objects/23.1 MiB.                                     \n"
          ]
        }
      ],
      "source": [
        "! gsutil cp $training_data_filename $evaluation_data_filename $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff68wmzoaalN"
      },
      "source": [
        "You can check to make sure that the file successfully transferred to your Google Cloud Storage bucket:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-DnKpYlaalN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6e62544-0e8a-419d-eb52-d44e1e4f667a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  23555736  2024-12-31T16:28:45Z  gs://vertex-eg6hgxqq/tune_data_stack_overflow_python_qa.jsonl#1735662525716654  metageneration=1\n",
            "    702058  2024-12-31T16:28:46Z  gs://vertex-eg6hgxqq/tune_eval_data_stack_overflow_python_qa.jsonl#1735662526285755  metageneration=1\n",
            "TOTAL: 2 objects, 24257794 bytes (23.13 MiB)\n"
          ]
        }
      ],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wE9P7OFaalN"
      },
      "outputs": [],
      "source": [
        "TRAINING_DATA_URI = f\"{BUCKET_URI}/{training_data_filename}\"\n",
        "EVALUATION_DATA_URI = f\"{BUCKET_URI}/{evaluation_data_filename}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mW7K57BaalN"
      },
      "source": [
        "### Model Tuning\n",
        "Now it's time to start to tune a model. You will use the Vertex AI SDK to submit our tuning job.\n",
        "\n",
        "#### Recommended Tuning Configurations\n",
        "‚úÖ Here are some recommended configurations for tuning a foundation model based on the task, in this example Q&A. You can find more in the [documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models).\n",
        "\n",
        "Extractive QA:\n",
        "- Make sure that your train dataset size is 100+\n",
        "- Training steps [100-500]. You can try more than one value to get the best performance on a particular dataset (e.g. 100, 200, 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vP_jeATTnbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da1439e9-6dda-445c-b42c-fe9fe6d18bcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.tensorboard.tensorboard_resource:Creating Tensorboard\n",
            "INFO:google.cloud.aiplatform.tensorboard.tensorboard_resource:Create Tensorboard backing LRO: projects/688769156274/locations/us-central1/tensorboards/3073755124191985664/operations/7780303604249788416\n",
            "INFO:google.cloud.aiplatform.tensorboard.tensorboard_resource:Tensorboard created. Resource name: projects/688769156274/locations/us-central1/tensorboards/3073755124191985664\n",
            "INFO:google.cloud.aiplatform.tensorboard.tensorboard_resource:To use this Tensorboard in another session:\n",
            "INFO:google.cloud.aiplatform.tensorboard.tensorboard_resource:tb = aiplatform.Tensorboard('projects/688769156274/locations/us-central1/tensorboards/3073755124191985664')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adapter tuning - \n",
            "projects/688769156274/locations/us-central1/tensorboards/3073755124191985664\n"
          ]
        }
      ],
      "source": [
        "# create tensorboard\n",
        "display_name = \"Adapter tuning - \"\n",
        "\n",
        "tensorboard = aiplatform.Tensorboard.create(\n",
        "    display_name=display_name,\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        ")\n",
        "\n",
        "print(tensorboard.display_name)\n",
        "print(tensorboard.resource_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CFVBoFu5Cnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c08891d-bb2f-45a6-e9fc-b2863124877d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3073755124191985664\n"
          ]
        }
      ],
      "source": [
        "# Get tensorboard_id thats used in the pipeline\n",
        "tensorboard_id = tensorboard.resource_name.split(\"tensorboards/\")[-1]\n",
        "print(tensorboard_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26HRfld3aalN"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = f\"gemini-1.0-pro-{UUID}\"\n",
        "TRAINING_STEPS = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvG1Rp3-iAG_"
      },
      "outputs": [],
      "source": [
        "pipeline_arguments = {\n",
        "    \"model_display_name\": MODEL_NAME,\n",
        "    \"location\": REGION,\n",
        "    \"large_model_reference\": \"gemini-1.0-pro\",\n",
        "    \"project\": PROJECT_ID,\n",
        "    \"train_steps\": TRAINING_STEPS,\n",
        "    \"dataset_uri\": TRAINING_DATA_URI,\n",
        "    \"evaluation_interval\": 20,\n",
        "    \"evaluation_data_uri\": EVALUATION_DATA_URI,\n",
        "    \"tensorboard_resource_id\": tensorboard_id,\n",
        "}\n",
        "\n",
        "pipeline_root = f\"{BUCKET_URI}/{MODEL_NAME}\"\n",
        "template_path = \"https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-model/v2.0.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on4baTh5aalN"
      },
      "outputs": [],
      "source": [
        "# Function that starts the tuning job\n",
        "\n",
        "\n",
        "def tuned_model(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    template_path: str,\n",
        "    model_display_name: str,\n",
        "    pipeline_arguments: str,\n",
        "):\n",
        "    \"\"\"Prompt-tune a new model, based on a prompt-response data.\n",
        "\n",
        "    \"training_data\" can be either the GCS URI of a file formatted in JSONL format\n",
        "    (for example: training_data=f'gs://{bucket}/{filename}.jsonl'), or a pandas\n",
        "    DataFrame. Each training example should be JSONL record with two keys, for\n",
        "    example:\n",
        "      {\n",
        "        \"input_text\": <input prompt>,\n",
        "        \"output_text\": <associated output>\n",
        "      },\n",
        "\n",
        "    Args:\n",
        "      project_id: Google Cloud Project ID, used to initialize aiplatform\n",
        "      location: Google Cloud Region, used to initialize aiplatform\n",
        "      template_path: path to the template\n",
        "      model_display_name: Name for your model.\n",
        "      pipeline_arguments: arguments used during pipeline runtime\n",
        "    \"\"\"\n",
        "\n",
        "    aiplatform.init(project=project_id, location=location)\n",
        "\n",
        "    from google.cloud.aiplatform import PipelineJob\n",
        "\n",
        "    job = PipelineJob(\n",
        "        template_path=template_path,\n",
        "        display_name=model_display_name,\n",
        "        parameter_values=pipeline_arguments,\n",
        "        location=REGION,\n",
        "        pipeline_root=pipeline_root,\n",
        "        enable_caching=True,\n",
        "    )\n",
        "\n",
        "    return job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0XNL9ojaalN"
      },
      "source": [
        "Next, it's time to start your tuning job.\n",
        "\n",
        "**Disclaimer:** tuning and deploying a model takes time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0m86z20zFgl"
      },
      "outputs": [],
      "source": [
        "job = tuned_model(PROJECT_ID, REGION, template_path, MODEL_NAME, pipeline_arguments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPHoXo8UIhlz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "899ddf6f-b281-4536-9971-f56f8dc9db5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FailedPrecondition",
          "evalue": "400 Bison model tuning is deprecated. Please migrate to Gemini tuning.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    276\u001b[0m     ) -> Any:\n\u001b[0;32m--> 277\u001b[0;31m         response, ignored_call = self._with_call(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36m_with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    331\u001b[0m         )\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;34m\"\"\"See grpc.Future.result.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36mcontinuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 response, call = self._thunk(new_method).with_call(\n\u001b[0m\u001b[1;32m    316\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36mwith_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1197\u001b[0m         )\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=not-instantiable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.FAILED_PRECONDITION\n\tdetails = \"Bison model tuning is deprecated. Please migrate to Gemini tuning.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:74.125.142.95:443 {grpc_message:\"Bison model tuning is deprecated. Please migrate to Gemini tuning.\", grpc_status:9, created_time:\"2024-12-31T17:20:05.872011793+00:00\"}\"\n>",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-8c2aeb98817b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, service_account, network, reserved_ip_ranges, create_request_timeout, experiment, enable_preflight_validations)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             self._gca_resource = self.api_client.create_pipeline_job(\n\u001b[0m\u001b[1;32m    513\u001b[0m                 \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0mpipeline_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/pipeline_service/client.py\u001b[0m in \u001b[0;36mcreate_pipeline_job\u001b[0;34m(self, request, parent, pipeline_job, pipeline_job_id, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m   1648\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPrecondition\u001b[0m: 400 Bison model tuning is deprecated. Please migrate to Gemini tuning."
          ]
        }
      ],
      "source": [
        "job.submit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRCkdxXvaalO"
      },
      "source": [
        "Following the link above, you can view your pipeline run. As you can see in the screenshot below, it will execute the following steps:\n",
        "\n",
        "- Validation\n",
        "- Export managed dataset\n",
        "- Convert JSONL to TFRecord\n",
        "- Large language model tuning\n",
        "- Upload LLM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq9rL5o32EXs"
      },
      "source": [
        "`job.state` lets you check the state of your pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8l8o3z30WO4"
      },
      "outputs": [],
      "source": [
        "job.state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6JC8XplaalO"
      },
      "source": [
        "## View your tuned foundational model on Vertex AI Model registry\n",
        "When your tuning job is finished, your model will be available on Vertex AI Model Registry. The following Python SDK sample shows you how to list tuned models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPWX0ITCaalO"
      },
      "outputs": [],
      "source": [
        "def list_tuned_models(project_id, location):\n",
        "    aiplatform.init(project=project_id, location=location)\n",
        "    model = TextGenerationModel.from_pretrained(\"text-bison\")\n",
        "    tuned_model_names = model.list_tuned_model_names()\n",
        "    print(tuned_model_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAIwCGYJaalO"
      },
      "outputs": [],
      "source": [
        "list_tuned_models(PROJECT_ID, REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZriyF0V-aalO"
      },
      "source": [
        "You can also use the Google Cloud Console UI to view all of your model in [Vertex AI Model Registry](https://console.cloud.google.com/vertex-ai/models). Below you can see an example of a tuned foundational model available on Vertex AI Model Registry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFftY6-EaalO"
      },
      "source": [
        "## Use your tuned model to get predictions\n",
        "Now it's time to get predictions. First you need to get the latest tuned model from the Vertex AI Model registry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU-K3EIkaalO"
      },
      "outputs": [],
      "source": [
        "def fetch_model(project_id, location):\n",
        "    aiplatform.init(project=project_id, location=location)\n",
        "    model = TextGenerationModel.from_pretrained(\"text-bison\")\n",
        "    list_tuned_models = model.list_tuned_model_names()\n",
        "    tuned_model = list_tuned_models[0]\n",
        "\n",
        "    return tuned_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j66dr12taalO"
      },
      "outputs": [],
      "source": [
        "deployed_model = fetch_model(PROJECT_ID, REGION)\n",
        "deployed_model = TextGenerationModel.get_tuned_model(deployed_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDOueoptaalO"
      },
      "source": [
        "Now you can start send a prompt to the API. Feel free to update the following prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ERbfPJPaalO"
      },
      "outputs": [],
      "source": [
        "PROMPT = \"\"\"\n",
        "How can I store my TensorFlow checkpoint on Google Cloud Storage?\n",
        "\n",
        "Python example:\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trzon4EyaalO"
      },
      "outputs": [],
      "source": [
        "print(deployed_model.predict(PROMPT))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtYr_KNPaalO"
      },
      "source": [
        "## Evaluation\n",
        "It's essential to evaluate your model to understand its performance. Evaluation can be done in an automated way using evaluation metrics like F1 or Rouge. You can also leverage human evaluation methods. Human evaluation methods involve asking humans to rate the quality of the LLM's answers. This can be done through crowdsourcing or by having experts evaluate the responses. Some standard human evaluation metrics include fluency, coherence, relevance, and informativeness. Often you want to choose a mix of evaluation metrics to get a good understanding of your model performance. Below you will find an example of how you can do the evaluation.\n",
        "\n",
        "In this example you will be using [sequence-evaluate](https://pypi.org/project/sequence-evaluate/) to evaluation the tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9856CuicaalO"
      },
      "outputs": [],
      "source": [
        "from seq_eval import SeqEval\n",
        "\n",
        "evaluator = SeqEval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS10ybdraalO"
      },
      "source": [
        "Earlier in the notebook, you created a train and eval dataset. Now it's time to take some of the eval data. You will use the questions to get a response from our tuned model, and the answers we will use as a reference:\n",
        "\n",
        "- **Candidates**: Answers generated by the tuned model.\n",
        "- **References**: Original answers that we will use to compare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKMmIH0XaalO"
      },
      "outputs": [],
      "source": [
        "evaluation = evaluation.head(10)  # you can change the number of rows you want to use\n",
        "evaluation_question = evaluation[\"input_text\"]\n",
        "evaluation_answer = evaluation[\"output_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx-g2molaalP"
      },
      "source": [
        "Now you can go ahead and generate candidates using the tuned model based on the questions you took from the eval dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5DqVXvEaalP"
      },
      "outputs": [],
      "source": [
        "candidates = []\n",
        "\n",
        "for i in evaluation_question:\n",
        "    response = deployed_model.predict(i)\n",
        "    candidates.append(response.text)\n",
        "\n",
        "len(candidates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oftLTb0maalP"
      },
      "source": [
        "You will also have to create a list of our references. These will you use to evaluate the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7zN70CJaalP"
      },
      "outputs": [],
      "source": [
        "references = evaluation_answer.tolist()\n",
        "\n",
        "len(references)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwKcyIDdjziD"
      },
      "source": [
        "Next you will generate the evaluation metrics. `evaluator.evaluate` will return a few eval metrics. Some of the important ones are:\n",
        "- [Blue](https://en.wikipedia.org/wiki/BLEU): The BLEU evaluation metric is a measure of the similarity between a machine-generated text and a human-written reference text.\n",
        "- [Rouge](https://en.wikipedia.org/wiki/ROUGE_(metric)): The ROUGE evaluation metric is a measure of the overlap between a machine-generated text and a human-written reference text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B828sNxUaalP"
      },
      "outputs": [],
      "source": [
        "scores = evaluator.evaluate(candidates, references, verbose=False)\n",
        "print(scores)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}